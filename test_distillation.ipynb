{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-26 18:57:05.115011: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-26 18:57:05.115111: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-26 18:57:05.177373: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-26 18:57:05.293824: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-26 18:57:06.759879: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from collections import deque\n",
    "import datetime\n",
    "import logging\n",
    "import threading\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import torch.nn as nn\n",
    "import ultralytics\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from utils.models import Detector, EmbeddingModel\n",
    "from utils import common\n",
    "from utils.losses import v8DetectionLoss, torch_square_norm\n",
    "from utils.ops import non_max_suppression\n",
    "from utils.dataset import *\n",
    "from utils.tal import TaskAlignedAssigner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"datasets/train.csv\")\n",
    "id_list = X_train[\"id\"].tolist()\n",
    "prefix = \"datasets/card_images_small/\"\n",
    "id_list = list(map(lambda x: prefix + str(x) + \".jpg\", id_list))\n",
    "card_type = list(map(lambda x: x.lower().startswith(\"pendulum\"), X_train[\"type\"].tolist()))\n",
    "train_dataset = DecklistDataset(id_list, card_type)\n",
    "del X_train, card_type, id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = pd.read_csv(\"datasets/valid.csv\")\n",
    "id_list = X_valid[\"id\"].tolist()\n",
    "prefix = \"datasets/card_images_small/\"\n",
    "id_list = list(map(lambda x: prefix + str(x) + \".jpg\", id_list))\n",
    "card_type = list(map(lambda x: x.lower().startswith(\"pendulum\"), X_valid[\"type\"].tolist()))\n",
    "valid_dataset = DecklistDataset(id_list, card_type)\n",
    "del X_valid, card_type, id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load(\"best 278.pt\")\n",
    "model = Detector(1000)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "teach_preprocess = common.EmbeddingPreprocessor()\n",
    "teacher_model = EmbeddingModel()\n",
    "teacher_model.load(\"weights/embedding.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset.shuffle()\n",
    "test_image, test_label = next(iter(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite(\"test.png\", test_image[:,:,::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = next(model.parameters()).device\n",
    "torch_inputs = common.detector_preprocess(test_image).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_detect, pred_embedding = model(torch_inputs.unsqueeze(0))\n",
    "\n",
    "# post processing\n",
    "pred_embedding = [\n",
    "    xi.view(pred_embedding[0].shape[0], pred_embedding[0].shape[1], -1) for xi in pred_embedding\n",
    "]\n",
    "pred_embedding = torch.cat(pred_embedding, 2)\n",
    "pred_embedding = pred_embedding.transpose(-1, -2)\n",
    "\n",
    "pred_bbox = model.layer22._inference(pred_detect)\n",
    "bbox_pos, bbox_mask = non_max_suppression(pred_bbox)\n",
    "\n",
    "bbox_pred = bbox_pos[bbox_mask]\n",
    "embedding = pred_embedding[bbox_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 27, 21, 0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1, y1, x2, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer 'resizing_5' (type Resizing).\n\n{{function_node __wrapped__ResizeBilinear_device_/job:localhost/replica:0/task:0/device:GPU:0}} input image must be of non-zero size [Op:ResizeBilinear] name: \n\nCall arguments received by layer 'resizing_5' (type Resizing):\n  â€¢ inputs=tf.Tensor(shape=(0, 21, 3), dtype=uint8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     x1, y1, x2, y2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mround(bbox)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m     11\u001b[0m     crop_img \u001b[38;5;241m=\u001b[39m test_image[y1:y2, x1:x2, :]\n\u001b[0;32m---> 12\u001b[0m     crop_img \u001b[38;5;241m=\u001b[39m \u001b[43membedding_preprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcrop_img\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     teacher_inputs\u001b[38;5;241m.\u001b[39mappend(crop_img)\n\u001b[1;32m     14\u001b[0m teacher_inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mstack(teacher_inputs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/mnt/d/develop/yugioh_deck_builder/ai/utils/common.py:177\u001b[0m, in \u001b[0;36mEmbeddingPreprocessor.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/yugioh/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/yugioh/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:5883\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5882\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5883\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer 'resizing_5' (type Resizing).\n\n{{function_node __wrapped__ResizeBilinear_device_/job:localhost/replica:0/task:0/device:GPU:0}} input image must be of non-zero size [Op:ResizeBilinear] name: \n\nCall arguments received by layer 'resizing_5' (type Resizing):\n  â€¢ inputs=tf.Tensor(shape=(0, 21, 3), dtype=uint8)"
     ]
    }
   ],
   "source": [
    "teacher_batch_size = 8\n",
    "embedding_preprocessor = common.EmbeddingPreprocessor()\n",
    "for i, indexes in enumerate(\n",
    "    common.make_batch(range(bbox_pred.shape[0]), teacher_batch_size)):\n",
    "    teacher_inputs = []\n",
    "    for index in indexes:\n",
    "        bbox = bbox_pred[index]\n",
    "        image_index = int(bbox[0].item())\n",
    "        bbox = bbox[1:5].clamp_(min=0).detach().cpu().numpy()\n",
    "        x1, y1, x2, y2 = np.round(bbox).astype(np.int32)\n",
    "        crop_img = test_image[y1:y2, x1:x2, :]\n",
    "        crop_img = embedding_preprocessor(crop_img)\n",
    "        teacher_inputs.append(crop_img)\n",
    "    teacher_inputs = tf.stack(teacher_inputs, axis=0)\n",
    "    if len(teacher_inputs) == 0:\n",
    "        continue\n",
    "    teacher_embedding = teacher_model.pred(teacher_inputs, len(teacher_inputs))\n",
    "    teacher_embedding = torch.from_numpy(teacher_embedding)\n",
    "    teacher_embedding = teacher_embedding.to(\n",
    "        dtype=embedding.dtype, device=embedding.device\n",
    "    )\n",
    "    loss = torch_square_norm(teacher_embedding, embedding[[indexes]])\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for position in bbox_pred:\n",
    "    x1, y1, x2, y2 = position[:4]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.8062e+01,  0.0000e+00,  1.2840e+02,  1.6950e+01,  1.9036e-03],\n",
       "        [ 9.6674e+01,  0.0000e+00,  1.4357e+02,  1.5850e+01,  2.2066e-03],\n",
       "        [ 2.5973e+02,  0.0000e+00,  3.0351e+02,  1.4969e+01,  4.9136e-03],\n",
       "        [ 3.3057e+02,  0.0000e+00,  3.7658e+02,  1.5358e+01,  5.4335e-03],\n",
       "        [ 4.4765e+02,  0.0000e+00,  4.9815e+02,  1.4845e+01,  4.2637e-03],\n",
       "        [ 4.8040e+02,  0.0000e+00,  5.2959e+02,  1.5213e+01,  3.8684e-03],\n",
       "        [ 5.3193e+02,  2.3260e-03,  5.9810e+02,  1.4273e+01,  2.7017e-03],\n",
       "        [ 5.5071e+02,  0.0000e+00,  6.1921e+02,  1.3174e+01,  2.0581e-03],\n",
       "        [ 2.8265e+01,  6.1470e-02,  1.0735e+02,  4.7189e+01,  2.3038e-04],\n",
       "        [ 1.0432e+02,  3.4046e-01,  1.4465e+02,  2.7698e+01,  6.0792e-04],\n",
       "        [ 1.8566e+02,  7.8014e-01,  2.2426e+02,  2.5194e+01,  9.2594e-04],\n",
       "        [ 2.1035e+02,  9.0075e-01,  2.4697e+02,  2.4873e+01,  1.2091e-03],\n",
       "        [ 3.2170e+02,  3.1931e-01,  3.6427e+02,  2.6695e+01,  1.7295e-03],\n",
       "        [ 3.3751e+02,  2.8272e-01,  3.8082e+02,  2.6702e+01,  1.7400e-03],\n",
       "        [ 5.6041e+02,  0.0000e+00,  6.3098e+02,  3.1216e+01,  2.0312e-04],\n",
       "        [ 5.7057e+02,  0.0000e+00,  6.3470e+02,  2.9323e+01,  2.1962e-04],\n",
       "        [ 1.1940e+02,  5.7746e+00,  1.6945e+02,  5.2923e+01,  1.7725e-04],\n",
       "        [ 2.8280e+02,  1.9054e+00,  3.3426e+02,  4.9893e+01,  4.0904e-04],\n",
       "        [ 3.4638e+02,  4.9128e-01,  3.9970e+02,  5.1648e+01,  3.4071e-04],\n",
       "        [ 1.5679e+01,  3.8819e+00,  1.0712e+02,  7.9030e+01,  5.8014e-06],\n",
       "        [ 9.5881e+01,  1.1101e+01,  1.7124e+02,  6.8076e+01,  1.0260e-04],\n",
       "        [ 1.1304e+02,  1.1433e+01,  1.8676e+02,  6.6770e+01,  1.4556e-04],\n",
       "        [ 1.5475e+02,  1.1799e+01,  2.2783e+02,  6.4181e+01,  2.2749e-04],\n",
       "        [ 2.8006e+02,  9.8191e+00,  3.4768e+02,  6.0584e+01,  2.6839e-04],\n",
       "        [ 3.5127e+02,  1.0081e+01,  4.2123e+02,  6.2675e+01,  2.1532e-04],\n",
       "        [ 3.7442e+02,  1.0314e+01,  4.4620e+02,  6.3531e+01,  1.9534e-04],\n",
       "        [ 3.9689e+02,  1.0391e+01,  4.7024e+02,  6.4617e+01,  1.6561e-04],\n",
       "        [ 4.3502e+02,  1.0202e+01,  5.1024e+02,  6.5541e+01,  1.2438e-04],\n",
       "        [ 4.5796e+02,  9.6506e+00,  5.3400e+02,  6.6161e+01,  9.8818e-05],\n",
       "        [ 4.6581e+02,  9.1655e+00,  5.4208e+02,  6.6408e+01,  8.7484e-05],\n",
       "        [ 4.7312e+02,  8.2319e+00,  5.5036e+02,  6.7129e+01,  7.5022e-05],\n",
       "        [ 4.9598e+02,  5.4914e+00,  5.8228e+02,  6.8778e+01,  4.3076e-05],\n",
       "        [ 5.1126e+02,  4.8315e+00,  6.0230e+02,  7.0807e+01,  2.7895e-05],\n",
       "        [ 5.3194e+02,  5.2253e+00,  6.2091e+02,  7.0109e+01,  2.3359e-05],\n",
       "        [ 5.3936e+02,  6.8530e+00,  6.2550e+02,  6.9073e+01,  2.5525e-05],\n",
       "        [ 5.6071e+02,  8.1148e+00,  6.3807e+02,  6.6075e+01,  1.5291e-05],\n",
       "        [ 5.7346e+02,  7.5477e+00,  6.4411e+02,  8.9242e+01,  4.7185e-05],\n",
       "        [-4.7343e+01,  9.8254e-01,  7.1908e+01,  8.2339e+01,  8.5023e-06],\n",
       "        [ 2.9271e+00,  7.4349e+00,  7.9637e+01,  7.1074e+01,  1.3772e-05],\n",
       "        [ 4.8178e+01,  1.5857e+01,  1.3691e+02,  8.0807e+01,  2.3663e-05],\n",
       "        [ 1.0392e+02,  1.6443e+01,  1.8231e+02,  7.4528e+01,  2.0834e-04],\n",
       "        [ 1.1247e+02,  1.6409e+01,  1.8962e+02,  7.3623e+01,  2.6003e-04],\n",
       "        [ 1.4634e+02,  1.6334e+01,  2.2187e+02,  7.0502e+01,  4.6190e-04],\n",
       "        [ 1.5493e+02,  1.6399e+01,  2.2971e+02,  6.9934e+01,  4.8533e-04],\n",
       "        [ 1.7193e+02,  1.6431e+01,  2.4499e+02,  6.9360e+01,  5.2757e-04],\n",
       "        [ 1.9678e+02,  1.6132e+01,  2.6793e+02,  6.8287e+01,  5.6477e-04],\n",
       "        [ 2.1368e+02,  1.5971e+01,  2.8442e+02,  6.7900e+01,  5.8491e-04],\n",
       "        [ 2.3150e+02,  1.6093e+01,  3.0068e+02,  6.7634e+01,  5.8580e-04],\n",
       "        [ 2.3971e+02,  1.6077e+01,  3.0857e+02,  6.7649e+01,  5.7413e-04],\n",
       "        [ 3.0500e+02,  1.5546e+01,  3.7261e+02,  6.7619e+01,  4.5530e-04],\n",
       "        [ 3.2136e+02,  1.5752e+01,  3.8873e+02,  6.8075e+01,  4.5616e-04],\n",
       "        [ 3.3733e+02,  1.5903e+01,  4.0510e+02,  6.8316e+01,  4.4891e-04],\n",
       "        [ 3.7616e+02,  1.6402e+01,  4.4699e+02,  6.9675e+01,  4.2882e-04],\n",
       "        [ 3.8328e+02,  1.6442e+01,  4.5507e+02,  7.0335e+01,  4.1518e-04],\n",
       "        [ 3.9082e+02,  1.6575e+01,  4.6297e+02,  7.0658e+01,  3.9803e-04],\n",
       "        [ 4.0586e+02,  1.6707e+01,  4.7915e+02,  7.1221e+01,  3.3590e-04],\n",
       "        [ 4.1308e+02,  1.6703e+01,  4.8711e+02,  7.1648e+01,  3.1742e-04],\n",
       "        [ 4.2883e+02,  1.6791e+01,  5.0303e+02,  7.1957e+01,  2.8768e-04],\n",
       "        [ 4.3636e+02,  1.6718e+01,  5.1118e+02,  7.2000e+01,  2.6449e-04],\n",
       "        [ 4.4339e+02,  1.6593e+01,  5.1891e+02,  7.2325e+01,  2.4313e-04],\n",
       "        [ 4.5085e+02,  1.6588e+01,  5.2710e+02,  7.2443e+01,  2.2455e-04],\n",
       "        [ 5.1919e+02,  1.4974e+01,  6.1193e+02,  7.6846e+01,  4.5136e-05]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbox_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yugioh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
